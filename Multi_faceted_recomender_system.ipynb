{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIS0wNejenrnI6aK3ElRee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaghemanth/RecSys_Projects/blob/main/Multi_faceted_recomender_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the required Libraries"
      ],
      "metadata": {
        "id": "88LQAEhKKqDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3MYcKrUJ1xjE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import permutations\n",
        "import re\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# Added for plotting the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model:</b><br>\n",
        "This is a cascaded model which uses the best of all the models in a weighted combination.\n",
        "\n",
        "We have used-\n",
        "User-Based Filtering,  Item-to-Item Co-occurrence, Content-Based Filtering, Matrix Factorization (SVD) and  Fallback Models."
      ],
      "metadata": {
        "id": "9jFIlehdBB25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_models(df, decay_rate=0.01, n_components=20, n_iter=5,\n",
        "                 frequency_power=1.0):\n",
        "    \"\"\"\n",
        "    Builds all necessary models, with tunable internal parameters.\n",
        "\n",
        "    This function trains several recommendation models including:\n",
        "    - Time-Weighted User History\n",
        "    - Item-to-Item Co-occurrence\n",
        "    - Content-Based Filtering\n",
        "    - SVD Matrix Factorization\n",
        "    - Fallback Popularity Models\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The training dataframe with all historical orders.\n",
        "            Expected columns: 'Member', 'Order', 'SKU', 'Delivery Date', 'Name'.\n",
        "        decay_rate (float): The rate for the time-decay function. Higher values\n",
        "                            mean more recent purchases have a stronger influence.\n",
        "        n_components (int): The number of latent factors for SVD. Represents the\n",
        "                            dimensionality of the reduced space for users and items.\n",
        "        n_iter (int): The number of iterations for the SVD solver. More iterations\n",
        "                      can lead to better convergence but take longer.\n",
        "        frequency_power (float): A power to apply to the time-weighted scores\n",
        "                                 to adjust for purchase frequency. Values > 1.0\n",
        "                                 will amplify the scores of frequently purchased\n",
        "                                 items; values < 1.0 will dampen them.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing all the trained models.\n",
        "              Keys include 'user_history', 'co_occurrence', 'content',\n",
        "              'popularity', 'category_popularity', and 'svd'.\n",
        "    \"\"\"\n",
        "    print(\n",
        "        f\"Building models with decay={decay_rate}, n_comp={n_components}, \"\n",
        "        f\"n_iter={n_iter}, freq_power={frequency_power}...\"\n",
        "    )\n",
        "    # Ensure 'Delivery Date' is in datetime format\n",
        "    df['Delivery Date'] = pd.to_datetime(df['Delivery Date'], format='%d/%m/%y')\n",
        "\n",
        "    # Popular items - for default fall back\n",
        "    # Get a list of SKUs sorted by their overall purchase frequency\n",
        "    global_popular_skus = df['SKU'].value_counts().index.tolist()\n",
        "\n",
        "    # Time-Weighted User History\n",
        "    # Calculate days since the latest purchase date in the dataset\n",
        "    latest_date = df['Delivery Date'].max()\n",
        "    df['days_since_purchase'] = (latest_date - df['Delivery Date']).dt.days\n",
        "    # Calculate a time score based on the decay rate\n",
        "    df['time_score'] = np.exp(-decay_rate * df['days_since_purchase'])\n",
        "    # Aggregate time scores for each user-SKU pair\n",
        "    user_sku_scores = df.groupby(['Member', 'SKU'])['time_score'].sum().reset_index()\n",
        "\n",
        "    # Apply the frequency_power hyperparameter to adjust scores\n",
        "    user_sku_scores['time_score'] = np.power(\n",
        "        user_sku_scores['time_score'], frequency_power\n",
        "    )\n",
        "    # Convert user-SKU scores to a dictionary for quick lookup\n",
        "    user_history_scores = user_sku_scores.set_index(\n",
        "        ['Member', 'SKU']\n",
        "    )['time_score'].to_dict()\n",
        "\n",
        "    # Item-to-Item Co-occurrence\n",
        "    # Build a co-occurrence matrix based on items appearing in the same order\n",
        "    co_occurrence_model = defaultdict(Counter)\n",
        "    order_items = df.groupby('Order')['SKU'].apply(list)\n",
        "    for items in order_items:\n",
        "        # Consider all pairs of items within an order\n",
        "        for item1, item2 in permutations(items, 2):\n",
        "            co_occurrence_model[item1][item2] += 1\n",
        "\n",
        "    # Normalize co-occurrence counts by the maximum count to scale between 0 and 1\n",
        "    max_count = max(\n",
        "        (c.most_common(1)[0][1] for c in co_occurrence_model.values() if c),\n",
        "        default=0\n",
        "    )\n",
        "    normalized_co_occurrence_model = defaultdict(Counter)\n",
        "    if max_count > 0:\n",
        "        for item1, counters in co_occurrence_model.items():\n",
        "            for item2, count in counters.items():\n",
        "                normalized_co_occurrence_model[item1][item2] = count / max_count\n",
        "\n",
        "    # Content-Based Filtering\n",
        "    # Define keyword mappings for categories\n",
        "    CATEGORY_KEYWORDS = {\n",
        "        'vegetables': ['vegetables', 'brinjals', 'gourd', 'cucumber', 'f&v'],\n",
        "        'dals & pulses': ['dal', 'dals', 'beans', 'pulses', 'peanuts'],\n",
        "        'flour & grains': ['flour', 'flours', 'sooji', 'rava', 'maida',\n",
        "                           'basmati', 'avalakki', 'poha', 'rice', 'besan',\n",
        "                           'grains', 'oats'],\n",
        "        'spices & sweeteners': ['sugar', 'jaggery', 'spices', 'masalas',\n",
        "                                'honey', 'salt'],\n",
        "        'oils & ghee': ['oils', 'ghee', 'oil'],\n",
        "        'dairy & eggs': ['yogurt', 'lassi', 'curd', 'buttermilk', 'milk',\n",
        "                         'butter', 'cream', 'eggs'],\n",
        "        'bakery & cakes': ['bread', 'buns', 'pavs', 'cakes'],\n",
        "        'snacks & biscuits': ['chips', 'cookies', 'biscuits', 'namkeen',\n",
        "                              'wafers', 'snacks', 'snacky'],\n",
        "        'nuts & dry fruits': ['cashews', 'almonds', 'raisins', 'dry fruits'],\n",
        "        'instant food & noodles': ['vermicelli', 'noodles', 'pasta', 'pastas',\n",
        "                                   'ready mix', 'baking mixes'],\n",
        "        'sauces & spreads': ['sauces', 'paste', 'ketchup', 'spreads'],\n",
        "        'beverages': ['juices', 'coffee', 'drinks', 'water'],\n",
        "        'frozen & desserts': ['ice creams', 'desserts', 'frozen'],\n",
        "        'personal care': ['bandages', 'soaps', 'wash', 'hair oil', 'shaving',\n",
        "                          'razors', 'pads', 'polish', 'toothpaste', 'dyes',\n",
        "                          'body wash'],\n",
        "        'household & cleaning': ['repellent', 'foil', 'wrap', 'pooja',\n",
        "                                 'cleaners', 'phenyles', 'acids',\n",
        "                                 'agarbatti', 'detergent', 'brooms'],\n",
        "        'sweets & candy': ['sweets', 'rasagulla', 'gulab jamun', 'toffee',\n",
        "                           'candy'],\n",
        "        'health': ['health drinks', 'supplements'],\n",
        "        'baby care': ['diapers', 'wipes']\n",
        "    }\n",
        "    # Create a mapping from SKU to item name\n",
        "    sku_to_name = df[['SKU', 'Name']].drop_duplicates().set_index(\n",
        "        'SKU'\n",
        "    )['Name'].to_dict()\n",
        "    # Assign categories to SKUs based on keywords in their names\n",
        "    sku_to_category, category_to_skus = {}, defaultdict(list)\n",
        "    for sku, name in sku_to_name.items():\n",
        "        name_lower = name.lower()\n",
        "        assigned_category = 'other' # Default category\n",
        "        for category, keywords in CATEGORY_KEYWORDS.items():\n",
        "            if any(re.search(r'\\b' + keyword + r'\\b', name_lower)\n",
        "                   for keyword in keywords):\n",
        "                assigned_category = category\n",
        "                break\n",
        "        sku_to_category[sku] = assigned_category\n",
        "        if assigned_category != 'other':\n",
        "            category_to_skus[assigned_category].append(sku)\n",
        "\n",
        "    # Calculate item popularity to determine popular items within each category\n",
        "    item_popularity = df['SKU'].value_counts().to_dict()\n",
        "    category_popular_items = {\n",
        "        cat: sorted(\n",
        "            skus, key=lambda sku: item_popularity.get(sku, 0), reverse=True\n",
        "        ) for cat, skus in category_to_skus.items()\n",
        "    }\n",
        "\n",
        "    # SVD Matrix Factorization\n",
        "    # Create mappings for users and SKUs to integer IDs for matrix factorization\n",
        "    user_ids = {\n",
        "        user: i for i, user in enumerate(user_sku_scores['Member'].unique())\n",
        "    }\n",
        "    sku_ids = {\n",
        "        sku: i for i, sku in enumerate(user_sku_scores['SKU'].unique())\n",
        "    }\n",
        "    # Create a sparse matrix from user-SKU scores\n",
        "    sparse_matrix = csr_matrix((\n",
        "        user_sku_scores['time_score'],\n",
        "        (\n",
        "            user_sku_scores['Member'].map(user_ids),\n",
        "            user_sku_scores['SKU'].map(sku_ids)\n",
        "        )\n",
        "    ))\n",
        "\n",
        "    # Apply Truncated SVD to decompose the sparse matrix into user and item factors\n",
        "    svd = TruncatedSVD(n_components=n_components, n_iter=n_iter, random_state=42)\n",
        "    user_factors = svd.fit_transform(sparse_matrix)\n",
        "    item_factors = svd.components_.T\n",
        "    # Predict scores by multiplying user and item factors\n",
        "    predicted_scores = user_factors.dot(item_factors.T)\n",
        "    # Normalize predicted scores\n",
        "    normalizer = Normalizer()\n",
        "    predicted_scores = normalizer.fit_transform(predicted_scores)\n",
        "\n",
        "    # Store SVD results in a dictionary\n",
        "    svd_model = {\n",
        "        \"predictions\": predicted_scores,\n",
        "        \"user_map\": user_ids,\n",
        "        \"sku_map\": sku_ids,\n",
        "        \"reverse_sku_map\": {i: sku for sku, i in sku_ids.items()}\n",
        "    }\n",
        "\n",
        "    # Combine all trained models into a single dictionary\n",
        "    models = {\n",
        "        \"user_history\": user_history_scores,\n",
        "        \"co_occurrence\": normalized_co_occurrence_model,\n",
        "        \"content\": sku_to_category,\n",
        "        \"popularity\": global_popular_skus,\n",
        "        \"category_popularity\": category_popular_items,\n",
        "        \"svd\": svd_model\n",
        "    }\n",
        "    print(\"Models built successfully.\")\n",
        "    return models"
      ],
      "metadata": {
        "id": "fEclosnJ1zYV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_recommendations(orders_to_predict, order_to_member_map,\n",
        "                            items_in_orders, models, co_occurrence_weight,\n",
        "                            content_weight, svd_weight):\n",
        "    \"\"\"\n",
        "    Generates recommendations using a four-part hybrid scoring model.\n",
        "\n",
        "    This function combines scores from User-Based History, Item-to-Item Co-occurrence,\n",
        "    Content-Based Filtering, and SVD Matrix Factorization to generate a ranked\n",
        "    list of recommended SKUs for each order. It also includes fallback\n",
        "    mechanisms based on category and global popularity.\n",
        "\n",
        "    Args:\n",
        "        orders_to_predict (list): A list of order IDs for which to generate recommendations.\n",
        "        order_to_member_map (dict): A dictionary mapping order IDs to member IDs.\n",
        "        items_in_orders (dict): A dictionary mapping order IDs to a set of SKUs\n",
        "                                present in that order (used as the current cart).\n",
        "        models (dict): A dictionary containing all the trained models, as returned\n",
        "                       by the `build_models` function.\n",
        "        co_occurrence_weight (float): The weight to apply to scores from the\n",
        "                                      Item-to-Item Co-occurrence model.\n",
        "        content_weight (float): The weight to apply to scores from the\n",
        "                                Content-Based Filtering model.\n",
        "        svd_weight (float): The weight to apply to scores from the SVD model.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a\n",
        "              recommendation and contains 'Member', 'Order', and 'SKU'.\n",
        "    \"\"\"\n",
        "    # Unpack the models from the input dictionary\n",
        "    user_history, co_occurrence, sku_to_category, global_popular, \\\n",
        "    cat_popular, svd_model = (\n",
        "        models['user_history'], models['co_occurrence'], models['content'],\n",
        "        models['popularity'], models['category_popularity'], models['svd']\n",
        "    )\n",
        "\n",
        "    final_recommendations = []\n",
        "    # Iterate through each order for which recommendations are needed\n",
        "    for order_id in orders_to_predict:\n",
        "        member_id = order_to_member_map[order_id]\n",
        "        items_in_cart = items_in_orders.get(order_id, set())\n",
        "        candidate_scores = Counter() # To accumulate scores for potential recommendations\n",
        "\n",
        "        # User Based History Scoring\n",
        "        # Get all items previously purchased by the current member\n",
        "        all_user_items = [\n",
        "            sku for (mem, sku) in user_history.keys() if mem == member_id\n",
        "        ]\n",
        "        # Add time-weighted scores from user history to candidate scores\n",
        "        for sku in all_user_items:\n",
        "            candidate_scores[sku] += user_history.get((member_id, sku), 0)\n",
        "\n",
        "        # Item-to-Item Co-occurrence Scoring\n",
        "        # For each item in the current order (cart), add scores of related items\n",
        "        for item_in_cart in items_in_cart:\n",
        "            if item_in_cart in co_occurrence:\n",
        "                for related_item, norm_count in \\\n",
        "                        co_occurrence[item_in_cart].items():\n",
        "                    # Apply co-occurrence weight\n",
        "                    candidate_scores[related_item] += \\\n",
        "                        co_occurrence_weight * norm_count\n",
        "\n",
        "        # Content-Based Scoring\n",
        "        # Identify categories present in the current order (cart)\n",
        "        session_cats = Counter(\n",
        "            sku_to_category.get(sku, 'other') for sku in items_in_cart\n",
        "        )\n",
        "        if session_cats:\n",
        "            # Get the count of the most frequent category in the cart\n",
        "            max_cat_count = session_cats.most_common(1)[0][1]\n",
        "            # For each potential recommendation (from user history),\n",
        "            # add a score based on its category's frequency in the cart\n",
        "            for cand_sku in all_user_items:\n",
        "                cand_cat = sku_to_category.get(cand_sku, 'other')\n",
        "                if cand_cat in session_cats and cand_cat != 'other':\n",
        "                    # Apply content weight and normalize by max category count\n",
        "                    candidate_scores[cand_sku] += \\\n",
        "                        content_weight * (session_cats[cand_cat] / max_cat_count)\n",
        "\n",
        "        # SVD Model Scoring\n",
        "        # Get the user index for the SVD model\n",
        "        user_idx = svd_model['user_map'].get(member_id)\n",
        "        if user_idx is not None:\n",
        "            # Get the predicted scores for the user from the SVD model\n",
        "            user_svd_scores = svd_model['predictions'][user_idx]\n",
        "            # Add SVD scores to candidate scores for each SKU\n",
        "            for sku_idx, score in enumerate(user_svd_scores):\n",
        "                sku = svd_model['reverse_sku_map'].get(sku_idx)\n",
        "                if sku:\n",
        "                    # Apply SVD weight\n",
        "                    candidate_scores[sku] += svd_weight * score\n",
        "\n",
        "        # Ranking and Filtering\n",
        "        # Remove items already in the current order (cart) from candidates\n",
        "        for item in items_in_cart:\n",
        "            if item in candidate_scores:\n",
        "                del candidate_scores[item]\n",
        "\n",
        "        # Get the top 5 recommended SKUs based on the combined scores\n",
        "        recs = [sku for sku, score in candidate_scores.most_common(5)]\n",
        "\n",
        "        # Default Fallback Items\n",
        "        # Keep track of all SKUs considered (recommended or in cart)\n",
        "        seen_skus = set(recs).union(items_in_cart)\n",
        "        # If less than 5 recommendations are generated, use fallback\n",
        "        if len(recs) < 5:\n",
        "            # First fallback: popular items from the most frequent categories in the cart\n",
        "            sorted_session_cats = [\n",
        "                cat for cat, count in session_cats.most_common()\n",
        "            ]\n",
        "            for cat in sorted_session_cats:\n",
        "                if len(recs) >= 5:\n",
        "                    break\n",
        "                if cat in cat_popular:\n",
        "                    for sku in cat_popular[cat]:\n",
        "                        if len(recs) >= 5:\n",
        "                            break\n",
        "                        if sku not in seen_skus:\n",
        "                            recs.append(sku)\n",
        "                            seen_skus.add(sku)\n",
        "        # Second fallback: globally popular items\n",
        "        if len(recs) < 5:\n",
        "            for sku in global_popular:\n",
        "                if len(recs) >= 5:\n",
        "                    break\n",
        "                if sku not in seen_skus:\n",
        "                    recs.append(sku)\n",
        "                    seen_skus.add(sku)\n",
        "\n",
        "        # Format the final recommendations for the submission file\n",
        "        for sku in recs:\n",
        "            final_recommendations.append(\n",
        "                {'Member': member_id, 'Order': order_id, 'SKU': sku}\n",
        "            )\n",
        "\n",
        "    return final_recommendations"
      ],
      "metadata": {
        "id": "Ul5hpj6414VO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_recall_at_5(train_df, test_orders_df, forgotten_item_fraction=0.3,\n",
        "                     decay_rate=0.01, co_occurrence_weight=0.5,\n",
        "                     content_weight=0.2, svd_weight=0.5, n_components=20,\n",
        "                     n_iter=5, frequency_power=1.0):\n",
        "    \"\"\"\n",
        "    Tests the recommendation model's recall@5 and calculates confusion matrix values.\n",
        "\n",
        "    This function simulates a \"Did You Forget\" scenario by hiding a fraction\n",
        "    of items from test orders and checking if the model recommends them.\n",
        "    It calculates Recall@5 and confusion matrix components (TP, FP, FN, TN).\n",
        "\n",
        "    Args:\n",
        "        train_df (pd.DataFrame): DataFrame containing historical orders for training.\n",
        "        test_orders_df (pd.DataFrame): DataFrame containing the orders to be used for testing.\n",
        "        forgotten_item_fraction (float): The fraction of items to hide from each\n",
        "                                         test order to simulate forgotten items.\n",
        "        decay_rate (float): Hyperparameter for the time-weighted user history model.\n",
        "        co_occurrence_weight (float): Hyperparameter for the item-to-item co-occurrence model.\n",
        "        content_weight (float): Hyperparameter for the content-based filtering model.\n",
        "        svd_weight (float): Hyperparameter for the SVD model.\n",
        "        n_components (int): Hyperparameter for the number of SVD components.\n",
        "        n_iter (int): Hyperparameter for the number of SVD iterations.\n",
        "        frequency_power (float): Hyperparameter for adjusting time-weighted scores\n",
        "                                 by purchase frequency.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "               - recall (float): The calculated Recall@5 score.\n",
        "               - total_tp (int): Total True Positives across all test orders.\n",
        "               - total_fp (int): Total False Positives across all test orders.\n",
        "               - total_fn (int): Total False Negatives across all test orders.\n",
        "               - total_tn (int): Total True Negatives across all test orders.\n",
        "    \"\"\"\n",
        "    print(\n",
        "        f\"\\n--- Testing with decay={decay_rate}, co_occur={co_occurrence_weight}, \"\n",
        "        f\"content={content_weight}, svd={svd_weight}, n_comp={n_components}, \"\n",
        "        f\"n_iter={n_iter}, freq_pow={frequency_power}, \"\n",
        "        f\"forgotten={forgotten_item_fraction} ---\"\n",
        "    )\n",
        "\n",
        "    # Build the recommendation models using the training data and specified hyperparameters\n",
        "    models = build_models(\n",
        "        train_df.copy(), decay_rate=decay_rate, n_components=n_components,\n",
        "        n_iter=n_iter, frequency_power=frequency_power\n",
        "    )\n",
        "    # Get unique order IDs from the test set\n",
        "    test_orders_to_predict = test_orders_df['Order'].unique()\n",
        "    # Create a mapping from order ID to member ID for the test set\n",
        "    order_to_member_map = test_orders_df.set_index('Order')['Member'].to_dict()\n",
        "    # Initialize dictionaries to store items kept in the cart and forgotten items (ground truth)\n",
        "    items_in_test_orders_subset, forgotten_items_ground_truth = {}, {}\n",
        "\n",
        "    # Create a lookup for all items ever purchased by each member in the training data\n",
        "    all_member_items = train_df.groupby('Member')['SKU'].apply(set).to_dict()\n",
        "\n",
        "    # Simulate forgotten items for each test order\n",
        "    for order_id in test_orders_to_predict:\n",
        "        all_items_in_order = test_orders_df[\n",
        "            test_orders_df['Order'] == order_id\n",
        "        ]['SKU'].tolist()\n",
        "        np.random.shuffle(all_items_in_order) # Randomly shuffle items\n",
        "        # Determine the number of items to hide based on the fraction\n",
        "        num_to_hide = int(len(all_items_in_order) * forgotten_item_fraction)\n",
        "        # Ensure at least one item is hidden if the order has more than one item\n",
        "        if num_to_hide == 0 and len(all_items_in_order) > 1:\n",
        "            num_to_hide = 1\n",
        "        # Split items into forgotten (hidden) and items kept in the cart\n",
        "        forgotten_items = set(all_items_in_order[:num_to_hide])\n",
        "        items_to_keep = set(all_items_in_order[num_to_hide:])\n",
        "        # Store the items kept in the cart and the forgotten items\n",
        "        items_in_test_orders_subset[order_id] = items_to_keep\n",
        "        forgotten_items_ground_truth[order_id] = forgotten_items\n",
        "\n",
        "    # Generate recommendations using the trained models and the items kept in the subset orders\n",
        "    recommendations_list = get_all_recommendations(\n",
        "        test_orders_to_predict, order_to_member_map,\n",
        "        items_in_test_orders_subset, models,\n",
        "        co_occurrence_weight=co_occurrence_weight,\n",
        "        content_weight=content_weight, svd_weight=svd_weight\n",
        "    )\n",
        "    # Group generated recommendations by order ID\n",
        "    recs_by_order = pd.DataFrame(recommendations_list).groupby(\n",
        "        'Order'\n",
        "    )['SKU'].apply(set).to_dict()\n",
        "\n",
        "    total_recall_score, orders_with_forgotten_items = 0, 0\n",
        "    # Initialize confusion matrix counters\n",
        "    total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
        "\n",
        "    # Evaluate recommendations against the forgotten items (ground truth)\n",
        "    for order_id in test_orders_to_predict:\n",
        "        forgotten_set = forgotten_items_ground_truth.get(order_id, set())\n",
        "        # Skip orders with no forgotten items (e.g., single-item orders where 0 items were hidden)\n",
        "        if not forgotten_set:\n",
        "            continue\n",
        "\n",
        "        member_id = order_to_member_map[order_id]\n",
        "        orders_with_forgotten_items += 1\n",
        "        # Get the set of recommended items for the current order\n",
        "        predicted_set = recs_by_order.get(order_id, set())\n",
        "\n",
        "        # Calculate recall for the current order\n",
        "        correctly_recalled_items = len(forgotten_set.intersection(predicted_set))\n",
        "        total_recall_score += correctly_recalled_items / len(forgotten_set)\n",
        "\n",
        "        # Calculate confusion matrix values for the current order\n",
        "        # Universe of items for TN calculation: all items the user has ever purchased\n",
        "        universe = all_member_items.get(member_id, set())\n",
        "        tp = correctly_recalled_items # True Positives: Forgotten items that were recommended\n",
        "        fn = len(forgotten_set.difference(predicted_set)) # False Negatives: Forgotten items that were NOT recommended\n",
        "        fp = len(predicted_set.difference(forgotten_set)) # False Positives: Recommended items that were NOT forgotten\n",
        "        # TN: Items in user's history, not forgotten, and not predicted\n",
        "        tn = len(universe - forgotten_set - predicted_set)\n",
        "\n",
        "        # Aggregate confusion matrix values\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "        total_tn += tn\n",
        "\n",
        "    # Calculate the overall Recall@5\n",
        "    recall = total_recall_score / orders_with_forgotten_items if orders_with_forgotten_items > 0 else 0\n",
        "\n",
        "    # Return recall and aggregated confusion matrix values\n",
        "    return recall, total_tp, total_fp, total_fn, total_tn"
      ],
      "metadata": {
        "id": "k7a0WaV417Fx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyperparameters(all_orders_path):\n",
        "    \"\"\"\n",
        "    Runs the recall test across a grid of hyperparameters, summarizes the\n",
        "    results, and plots a confusion matrix for the best model.\n",
        "\n",
        "    This function orchestrates the hyperparameter tuning process by iterating\n",
        "    through predefined grids of hyperparameters, evaluating the model's\n",
        "    performance using the `test_recall_at_5` function, and reporting the\n",
        "    best performing set of parameters based on Recall@5. It also visualizes\n",
        "    the confusion matrix for the best model.\n",
        "\n",
        "    Args:\n",
        "        all_orders_path (str): The file path to the CSV file containing all\n",
        "                               historical order data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the grid of hyperparameters to search over\n",
        "    decay_rate_grid = [0.0003]\n",
        "    co_occurrence_weight_grid = [0.5]\n",
        "    content_weight_grid = [0.5]\n",
        "    svd_weight_grid = [0.5]\n",
        "    n_components_grid = [70]\n",
        "    n_iter_grid = [5]\n",
        "    frequency_power_grid = [1.3]\n",
        "    forgotten_fraction_grid = [0.25] # Fraction of items to \"forget\" in test orders\n",
        "\n",
        "    results = [] # List to store results for each hyperparameter combination\n",
        "    print(f\"Loading data from '{all_orders_path}' for tuning\")\n",
        "    try:\n",
        "        df = pd.read_csv(all_orders_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}.\")\n",
        "        return\n",
        "\n",
        "    # Prepare data for training and testing\n",
        "    df['Delivery Date'] = pd.to_datetime(df['Delivery Date'], format='%d/%m/%y')\n",
        "    # Identify the last order for each member to use as the test set\n",
        "    last_order_info = df.loc[df.groupby('Member')['Delivery Date'].idxmax()]\n",
        "    test_orders_df = df[df['Order'].isin(last_order_info['Order'])]\n",
        "    # Use all orders except the last one for each member as the training set\n",
        "    train_df = df[~df['Order'].isin(last_order_info['Order'])]\n",
        "    # print(\n",
        "    #     f\"Training set size: {len(train_df)} rows, \"\n",
        "    #     f\"Test set size: {len(test_orders_df)} rows\"\n",
        "    # )\n",
        "\n",
        "    # Iterate through the hyperparameter grid and run the test for each combination\n",
        "    for decay_rate in decay_rate_grid:\n",
        "        for co_occurrence_weight in co_occurrence_weight_grid:\n",
        "            for content_weight in content_weight_grid:\n",
        "                for svd_weight in svd_weight_grid:\n",
        "                    for n_components in n_components_grid:\n",
        "                        for n_iter in n_iter_grid:\n",
        "                            for frequency_power in frequency_power_grid:\n",
        "                                for forgotten_fraction in forgotten_fraction_grid:\n",
        "                                    # Run the test and get recall and confusion matrix values\n",
        "                                    recall, tp, fp, fn, tn = test_recall_at_5(\n",
        "                                        train_df.copy(), test_orders_df.copy(),\n",
        "                                        forgotten_item_fraction=forgotten_fraction,\n",
        "                                        decay_rate=decay_rate,\n",
        "                                        co_occurrence_weight=co_occurrence_weight,\n",
        "                                        content_weight=content_weight,\n",
        "                                        svd_weight=svd_weight,\n",
        "                                        n_components=n_components,\n",
        "                                        n_iter=n_iter,\n",
        "                                        frequency_power=frequency_power\n",
        "                                    )\n",
        "                                    # Store the results\n",
        "                                    results.append({\n",
        "                                        'decay_rate': decay_rate,\n",
        "                                        'co_occurrence_weight': co_occurrence_weight,\n",
        "                                        'content_weight': content_weight,\n",
        "                                        'svd_weight': svd_weight,\n",
        "                                        'n_components': n_components,\n",
        "                                        'n_iter': n_iter,\n",
        "                                        'frequency_power': frequency_power,\n",
        "                                        'forgotten_fraction': forgotten_fraction,\n",
        "                                        'recall_at_5': recall,\n",
        "                                        # Store confusion matrix values\n",
        "                                        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn\n",
        "                                    })\n",
        "\n",
        "    # Analyze and display the results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\nHyperparameter Tuning Summary\")\n",
        "    # Sort results by Recall@5 in descending order\n",
        "    results_df.sort_values(by='recall_at_5', ascending=False, inplace=True)\n",
        "    print(results_df.to_string(index=False)) # Print the summary table\n",
        "\n",
        "    # Identify and display the best performing set of hyperparameters\n",
        "    best_result = results_df.iloc[0]\n",
        "    print(\"\\nBest Overall Result\")\n",
        "    print(best_result)\n",
        "\n",
        "    # Plot Confusion Matrix for the Best Result\n",
        "    print(\"\\n Confusion Matrix: Best Model\")\n",
        "    # Create a numpy array for the confusion matrix heatmap\n",
        "    cm_data = np.array([\n",
        "        [best_result['tn'], best_result['fp']],\n",
        "        [best_result['fn'], best_result['tp']]\n",
        "    ])\n",
        "\n",
        "    # Plot the confusion matrix using seaborn\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_data, annot=True, fmt='.0f', cmap='Blues',\n",
        "                xticklabels=['Not Recommended', 'Recommended'],\n",
        "                yticklabels=['Not Forgotten', 'Forgotten'])\n",
        "    plt.title('Confusion Matrix for Best Performing Model')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Generate the final submission file using the best hyperparameters\n",
        "    print(\"\\nGenerating final submission file with best hyperparameters...\")\n",
        "    generate_recommendations(\n",
        "        all_orders_path=all_orders_path,\n",
        "        last_orders_path='last_orders_subset.csv', # Assuming this is the path to the actual last orders for submission\n",
        "        output_path='Recsys_5_sets.csv', # Assuming this is the desired output path\n",
        "        decay_rate=best_result['decay_rate'],\n",
        "        co_occurrence_weight=best_result['co_occurrence_weight'],\n",
        "        content_weight=best_result['content_weight'],\n",
        "        svd_weight=best_result['svd_weight'],\n",
        "        n_components=int(best_result['n_components']), # Cast to int\n",
        "        n_iter=int(best_result['n_iter']), # Cast to int\n",
        "        frequency_power=best_result['frequency_power']\n",
        "    )"
      ],
      "metadata": {
        "id": "pDYUrItc1-J-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(all_orders_path, last_orders_path, output_path,\n",
        "                             group_number=1, decay_rate=0.01,\n",
        "                             co_occurrence_weight=0.5, content_weight=0.2,\n",
        "                             svd_weight=0.5, n_components=20, n_iter=5,\n",
        "                             frequency_power=1.0):\n",
        "    \"\"\"\n",
        "    Generates the final recommendation file for submission.\n",
        "\n",
        "    This function loads the historical and last order data, builds the\n",
        "    recommendation models using specified (or tuned) hyperparameters,\n",
        "    generates recommendations for the last orders, and saves the results\n",
        "    in a specified CSV format.\n",
        "\n",
        "    Args:\n",
        "        all_orders_path (str): The file path to the CSV file containing all\n",
        "                               historical order data (training data).\n",
        "        last_orders_path (str): The file path to the CSV file containing the\n",
        "                                last orders for which recommendations are needed.\n",
        "        output_path (str): The file path where the generated submission CSV\n",
        "                           file will be saved.\n",
        "        group_number (int): The group number (used in the output filename format).\n",
        "        decay_rate (float): Hyperparameter for the time-weighted user history model.\n",
        "        co_occurrence_weight (float): Hyperparameter for the item-to-item co-occurrence model.\n",
        "        content_weight (float): Hyperparameter for the content-based filtering model.\n",
        "        svd_weight (float): Hyperparameter for the SVD model.\n",
        "        n_components (int): Hyperparameter for the number of SVD components.\n",
        "        n_iter (int): Hyperparameter for the number of SVD iterations.\n",
        "        frequency_power (float): Hyperparameter for adjusting time-weighted scores\n",
        "                                 by purchase frequency.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the datasets\n",
        "    try:\n",
        "        all_orders_df = pd.read_csv(all_orders_path)\n",
        "        last_orders_df = pd.read_csv(last_orders_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(\n",
        "            f\"Files Not found: {e}\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # Build Recommendation Models using historical data\n",
        "    models = build_models(\n",
        "        all_orders_df, decay_rate=decay_rate, n_components=n_components,\n",
        "        n_iter=n_iter, frequency_power=frequency_power\n",
        "    )\n",
        "\n",
        "    # Prepare Last Order Data for prediction\n",
        "    # Create a mapping from order ID to member ID for the last orders\n",
        "    order_to_member_map = last_orders_df.set_index('Order')['Member'].to_dict()\n",
        "    # Group items by order ID to get the items in each last order (the \"cart\" for prediction)\n",
        "    items_in_last_orders = last_orders_df.groupby('Order')['SKU'].apply(set).to_dict()\n",
        "    # Get the unique order IDs for which to generate recommendations\n",
        "    orders_to_predict = last_orders_df['Order'].unique()\n",
        "    print(\n",
        "        f\"Found {len(orders_to_predict)} unique orders to predict for submission.\"\n",
        "    )\n",
        "\n",
        "    # Generate Recommendations for Each Order\n",
        "    final_recommendations = get_all_recommendations(\n",
        "        orders_to_predict, order_to_member_map, items_in_last_orders,\n",
        "        models, co_occurrence_weight=co_occurrence_weight,\n",
        "        content_weight=content_weight, svd_weight=svd_weight\n",
        "    )\n",
        "\n",
        "    # Create and Save the Submission File\n",
        "    print(\"Saving submission file\")\n",
        "    submission_df = pd.DataFrame(final_recommendations)\n",
        "    # Add an 'ID' column starting from 1\n",
        "    submission_df.reset_index(inplace=True)\n",
        "    submission_df.rename(columns={'index': 'ID'}, inplace=True)\n",
        "    submission_df[\"ID\"] = submission_df[\"ID\"] + 1\n",
        "\n",
        "    # Reorder columns to match the required submission format\n",
        "    submission_df = submission_df[['ID', 'Order', 'SKU', 'Member']]\n",
        "    # Save the DataFrame to a CSV file without the index\n",
        "    submission_df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "3FLGkVeZ2Ay8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to True to run the hyperparameter search and see the confusion matrix\n",
        "# Set to False to use predefined best hyperparameters and generate the submission file\n",
        "RUN_TUNING = True\n",
        "\n",
        "# --- Configuration ---\n",
        "# File paths for the input data and the output submission file\n",
        "ALL_ORDERS_FILE = 'all_except_last_orders.csv' # Path to the training data\n",
        "LAST_ORDERS_FILE = 'last_orders_subset.csv' # Path to the test/prediction data\n",
        "OUTPUT_FILE_FORMAT = 'Recsys_5_sets.csv' # Path for the output submission file\n",
        "\n",
        "# Conditional execution based on the RUN_TUNING flag\n",
        "if RUN_TUNING:\n",
        "    # If RUN_TUNING is True, execute the hyperparameter tuning process\n",
        "    # This will run the tuning process and find the best parameters,\n",
        "    # and plots the confusion matrix for the best model.\n",
        "    tune_hyperparameters(all_orders_path=ALL_ORDERS_FILE)\n",
        "else:\n",
        "    # If RUN_TUNING is False, use the predefined best hyperparameters\n",
        "    # These parameters were likely determined from a previous tuning run\n",
        "    best_decay_rate = 0.0003\n",
        "    best_co_occurrence_weight = 0.5\n",
        "    best_content_weight = 0.5\n",
        "    best_svd_weight = 0.5\n",
        "    best_n_components = 70\n",
        "    best_n_iter = 5\n",
        "    best_frequency_power = 1.3\n",
        "\n",
        "    # Generate the final recommendation file for submission using the best parameters\n",
        "    generate_recommendations(\n",
        "        all_orders_path=ALL_ORDERS_FILE,\n",
        "        last_orders_path=LAST_ORDERS_FILE,\n",
        "        output_path=OUTPUT_FILE_FORMAT,\n",
        "        decay_rate=best_decay_rate,\n",
        "        co_occurrence_weight=best_co_occurrence_weight,\n",
        "        content_weight=best_content_weight,\n",
        "        svd_weight=best_svd_weight,\n",
        "        n_components=best_n_components,\n",
        "        n_iter=best_n_iter,\n",
        "        frequency_power=best_frequency_power\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0vOnPna2C4f",
        "outputId": "5750e621-cc49-4904-dcce-39f6910e5a50"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building models with decay=0.0003, n_comp=70, n_iter=5, freq_power=1.3...\n",
            "Models built successfully.\n",
            "Found 638 unique orders to predict for submission.\n",
            "Saving submission file\n"
          ]
        }
      ]
    }
  ]
}